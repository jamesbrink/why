[package]
name = "why"
version = "0.1.0"
edition = "2021"
authors = ["James Brink <brink.james@gmail.com>"]
description = "Quick error explanation CLI using local LLM"
homepage = "https://github.com/jamesbrink/why"
repository = "https://github.com/jamesbrink/why"
license = "MIT"
readme = "README.md"
keywords = ["cli", "llm", "error-handling", "developer-tools", "ai"]
categories = ["command-line-utilities", "development-tools"]
autoexamples = false  # Don't try to compile example error scripts
build = "build.rs"

[features]
default = []
cuda = ["llama-cpp-2/cuda"]
vulkan = ["llama-cpp-2/vulkan"]
metal = ["llama-cpp-2/metal"]

[dependencies]
llama-cpp-2 = "0.1"
anyhow = "1.0"
encoding_rs = "0.8"
colored = "3"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
toml = "0.8"
regex = "1"
dirs = "6"
textwrap = { version = "0.16", features = ["terminal_size"] }
clap = { version = "4", features = ["derive"] }
clap_complete = "4"
notify = "8"
crossterm = "0.29"
ctrlc = "3.4"
libc = "0.2"

# HTTP client for external AI providers
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls", "json", "stream"] }
tokio = { version = "1", features = ["rt-multi-thread", "macros"] }
futures = "0.3"
async-trait = "0.1"

# Streaming support
tokio-stream = "0.1"
eventsource-stream = "0.2"

# Error handling
thiserror = "2"

[profile.release]
opt-level = "z"
lto = true
strip = true
